{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkGjr8ETyvqCAeAm8GwisP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SyedIftikharZaidi/Machine_Learning/blob/main/GradientDescant_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Steps For Writing Gradient Descant Algorithm**"
      ],
      "metadata": {
        "id": "vQAMAOWFGYcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we are bulding ML algorithms we need to find out the value of 'w' and 'b' for that we need to use the gradient descant algorithm.\n",
        "\n",
        "\n",
        "Gradient Descant For Linear Regression\n",
        "1.   yhat = wx + b\n",
        "2.   loss = (y-yhat)**2/N\n",
        "3.   Initialise some paratmeters\n",
        "4.   Create GD function\n",
        "5.   Iteratively makes update"
      ],
      "metadata": {
        "id": "-i020iqhGmgd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J6GPrqsAGObG"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Initialised Pararmeters**"
      ],
      "metadata": {
        "id": "RICatAdAGWgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are creating input data (x) and our target variable (y) by creating some random data"
      ],
      "metadata": {
        "id": "43P0RFtJJiYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.randn(10,1)\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHByQM1XICjz",
        "outputId": "2c76c34a-2670-4b30-c924-2b0278fd0906"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.08811048],\n",
              "       [ 2.45510356],\n",
              "       [-0.26924857],\n",
              "       [ 0.61393311],\n",
              "       [ 0.79854911],\n",
              "       [ 0.32728019],\n",
              "       [-0.06469006],\n",
              "       [-0.5236279 ],\n",
              "       [-0.36309236],\n",
              "       [ 0.25226907]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = 2*X + np.random.randn()"
      ],
      "metadata": {
        "id": "yO1ZJhfPIVPT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Initialize parameters**"
      ],
      "metadata": {
        "id": "NVFId7J7KoUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating w and b parameters\n",
        "w= 0.0\n",
        "b=0.0"
      ],
      "metadata": {
        "id": "HtclhxwJJBhT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "n358O40uKJXP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Create Gradient Descant Function**"
      ],
      "metadata": {
        "id": "52Ro8mf2K3DF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Partial Derivatives\n",
        "\n",
        "- **Partial derivative of Loss w.r.t \\( w \\):**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\text{Loss}}{\\partial w} = -2 \\sum_{i=1}^{N} x_i \\left( y_i - (w x_i + b) \\right)\n",
        "$$\n",
        "\n",
        "- **Partial derivative of Loss w.r.t \\( b \\):**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\text{Loss}}{\\partial b} = -2 \\sum_{i=1}^{N} \\left( y_i - (w x_i + b) \\right)\n",
        "$$\n",
        "\n",
        "### 2. Update Parameters\n",
        "\n",
        "- **Update for \\( w \\):**\n",
        "\n",
        "$$\n",
        "w_{\\text{new}} = w - \\eta \\frac{1}{N} \\frac{\\partial \\text{Loss}}{\\partial w}\n",
        "$$\n",
        "\n",
        "- **Update for \\( b \\):**\n",
        "\n",
        "$$\n",
        "b_{\\text{new}} = b - \\eta \\frac{1}{N} \\frac{\\partial \\text{Loss}}{\\partial b}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( w \\) is the weight (slope)\n",
        "- \\( b \\) is the bias (intercept)\n",
        "- \\( \\eta \\) is the learning rate\n",
        "- \\( N \\) is the number of data points\n",
        "- \\( x_i \\) and \\( y_i \\) are the input and target values respectively.\n"
      ],
      "metadata": {
        "id": "jYPaKAV2AsLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def descend(x,y,w,b,learning_rate):\n",
        "  dldw = 0.0 #partial derivative of w\n",
        "  dldb = 0.0 #partial derivative of b\n",
        "  N = x.shape[0]\n",
        "\n",
        "  for xi, yi in zip(x,y):\n",
        "    dldw += -2*xi*(yi-(w*xi+b)) #after taking the partial derivative of [loss = (-y(wx+b))**2 ]\n",
        "    dldb += -2*(yi-(w*xi+b)) #after taking the partial derivative of [loss = (-y(wx+b))**2 ]\n",
        "\n",
        "  W = w - learning_rate*(1/N)*dldw #making update in 'w' parameter\n",
        "  B = b - learning_rate*(1/N)*dldb #making update in 'b' parameter\n",
        "\n",
        "  return W,B\n"
      ],
      "metadata": {
        "id": "BT7kzz_BK2dj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Iteratively makes update**"
      ],
      "metadata": {
        "id": "KI-Oe9PfO7-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "  w, b = descend(X,y,w,b,learning_rate)\n",
        "  yhat = w*X + b\n",
        "  loss = np.divide(np.sum((y-yhat)**2, axis=0), X.shape[0])\n",
        "  print(f'{epoch} loss is {loss}, parameters w:{w}, b:{b}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtb6nioGO7Ng",
        "outputId": "76ccbb2b-4308-4530-96d5-63f745c2f59a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss is [6.6476481], parameters w:[0.04179641], b:[0.03791302]\n",
            "1 loss is [6.34720867], parameters w:[0.08268841], b:[0.07488903]\n",
            "2 loss is [6.06044219], parameters w:[0.12269607], b:[0.11095064]\n",
            "3 loss is [5.78672364], parameters w:[0.161839], b:[0.14611991]\n",
            "4 loss is [5.52545664], parameters w:[0.20013638], b:[0.1804184]\n",
            "5 loss is [5.27607217], parameters w:[0.23760694], b:[0.21386712]\n",
            "6 loss is [5.03802727], parameters w:[0.27426901], b:[0.24648662]\n",
            "7 loss is [4.81080387], parameters w:[0.31014049], b:[0.27829694]\n",
            "8 loss is [4.59390763], parameters w:[0.34523887], b:[0.30931763]\n",
            "9 loss is [4.38686688], parameters w:[0.37958127], b:[0.33956781]\n",
            "10 loss is [4.18923155], parameters w:[0.41318441], b:[0.36906611]\n",
            "11 loss is [4.00057219], parameters w:[0.44606463], b:[0.39783073]\n",
            "12 loss is [3.82047901], parameters w:[0.47823791], b:[0.42587944]\n",
            "13 loss is [3.64856102], parameters w:[0.50971986], b:[0.45322958]\n",
            "14 loss is [3.4844451], parameters w:[0.54052575], b:[0.47989808]\n",
            "15 loss is [3.32777525], parameters w:[0.5706705], b:[0.50590146]\n",
            "16 loss is [3.17821174], parameters w:[0.60016869], b:[0.53125585]\n",
            "17 loss is [3.03543043], parameters w:[0.62903459], b:[0.55597699]\n",
            "18 loss is [2.89912198], parameters w:[0.65728213], b:[0.58008026]\n",
            "19 loss is [2.76899124], parameters w:[0.68492494], b:[0.60358066]\n",
            "20 loss is [2.64475656], parameters w:[0.71197632], b:[0.62649283]\n",
            "21 loss is [2.52614917], parameters w:[0.73844931], b:[0.64883107]\n",
            "22 loss is [2.41291261], parameters w:[0.76435663], b:[0.67060932]\n",
            "23 loss is [2.30480215], parameters w:[0.78971072], b:[0.69184121]\n",
            "24 loss is [2.20158426], parameters w:[0.81452373], b:[0.71254003]\n",
            "25 loss is [2.10303609], parameters w:[0.83880757], b:[0.73271875]\n",
            "26 loss is [2.00894497], parameters w:[0.86257385], b:[0.75239005]\n",
            "27 loss is [1.91910798], parameters w:[0.88583393], b:[0.77156627]\n",
            "28 loss is [1.83333145], parameters w:[0.90859891], b:[0.7902595]\n",
            "29 loss is [1.7514306], parameters w:[0.93087967], b:[0.8084815]\n",
            "30 loss is [1.67322906], parameters w:[0.95268681], b:[0.82624377]\n",
            "31 loss is [1.59855855], parameters w:[0.97403071], b:[0.84355753]\n",
            "32 loss is [1.52725847], parameters w:[0.99492151], b:[0.86043373]\n",
            "33 loss is [1.45917555], parameters w:[1.01536914], b:[0.87688307]\n",
            "34 loss is [1.39416354], parameters w:[1.03538328], b:[0.89291597]\n",
            "35 loss is [1.33208285], parameters w:[1.05497343], b:[0.90854262]\n",
            "36 loss is [1.27280028], parameters w:[1.07414883], b:[0.92377295]\n",
            "37 loss is [1.21618872], parameters w:[1.09291857], b:[0.93861667]\n",
            "38 loss is [1.16212684], parameters w:[1.11129148], b:[0.95308324]\n",
            "39 loss is [1.11049888], parameters w:[1.12927624], b:[0.9671819]\n",
            "40 loss is [1.06119434], parameters w:[1.14688131], b:[0.98092168]\n",
            "41 loss is [1.01410777], parameters w:[1.16411497], b:[0.99431136]\n",
            "42 loss is [0.96913853], parameters w:[1.1809853], b:[1.00735955]\n",
            "43 loss is [0.92619058], parameters w:[1.19750024], b:[1.02007463]\n",
            "44 loss is [0.88517226], parameters w:[1.2136675], b:[1.03246477]\n",
            "45 loss is [0.84599608], parameters w:[1.22949466], b:[1.04453797]\n",
            "46 loss is [0.80857854], parameters w:[1.2449891], b:[1.05630202]\n",
            "47 loss is [0.77283996], parameters w:[1.26015807], b:[1.06776452]\n",
            "48 loss is [0.73870429], parameters w:[1.27500862], b:[1.07893289]\n",
            "49 loss is [0.70609894], parameters w:[1.28954768], b:[1.08981439]\n",
            "50 loss is [0.67495462], parameters w:[1.30378201], b:[1.10041607]\n",
            "51 loss is [0.64520522], parameters w:[1.3177182], b:[1.11074485]\n",
            "52 loss is [0.61678762], parameters w:[1.33136272], b:[1.12080745]\n",
            "53 loss is [0.58964157], parameters w:[1.34472189], b:[1.13061044]\n",
            "54 loss is [0.5637096], parameters w:[1.35780189], b:[1.14016024]\n",
            "55 loss is [0.53893682], parameters w:[1.37060876], b:[1.14946311]\n",
            "56 loss is [0.51527084], parameters w:[1.38314841], b:[1.15852514]\n",
            "57 loss is [0.49266168], parameters w:[1.39542661], b:[1.16735231]\n",
            "58 loss is [0.47106161], parameters w:[1.407449], b:[1.17595042]\n",
            "59 loss is [0.45042508], parameters w:[1.41922112], b:[1.18432515]\n",
            "60 loss is [0.4307086], parameters w:[1.43074835], b:[1.19248205]\n",
            "61 loss is [0.41187069], parameters w:[1.44203599], b:[1.2004265]\n",
            "62 loss is [0.39387172], parameters w:[1.45308919], b:[1.2081638]\n",
            "63 loss is [0.37667387], parameters w:[1.46391301], b:[1.21569907]\n",
            "64 loss is [0.36024105], parameters w:[1.47451238], b:[1.22303735]\n",
            "65 loss is [0.34453879], parameters w:[1.48489213], b:[1.23018353]\n",
            "66 loss is [0.32953421], parameters w:[1.49505698], b:[1.2371424]\n",
            "67 loss is [0.3151959], parameters w:[1.50501155], b:[1.24391862]\n",
            "68 loss is [0.30149389], parameters w:[1.51476036], b:[1.25051674]\n",
            "69 loss is [0.28839957], parameters w:[1.52430782], b:[1.25694121]\n",
            "70 loss is [0.27588561], parameters w:[1.53365825], b:[1.26319635]\n",
            "71 loss is [0.26392594], parameters w:[1.54281587], b:[1.2692864]\n",
            "72 loss is [0.25249566], parameters w:[1.55178481], b:[1.27521549]\n",
            "73 loss is [0.24157101], parameters w:[1.56056912], b:[1.28098763]\n",
            "74 loss is [0.2311293], parameters w:[1.56917274], b:[1.28660677]\n",
            "75 loss is [0.22114885], parameters w:[1.57759953], b:[1.29207673]\n",
            "76 loss is [0.211609], parameters w:[1.58585328], b:[1.29740125]\n",
            "77 loss is [0.20249], parameters w:[1.59393768], b:[1.30258398]\n",
            "78 loss is [0.19377299], parameters w:[1.60185634], b:[1.30762848]\n",
            "79 loss is [0.18543999], parameters w:[1.6096128], b:[1.31253822]\n",
            "80 loss is [0.17747379], parameters w:[1.61721051], b:[1.3173166]\n",
            "81 loss is [0.169858], parameters w:[1.62465286], b:[1.32196692]\n",
            "82 loss is [0.16257695], parameters w:[1.63194315], b:[1.3264924]\n",
            "83 loss is [0.15561569], parameters w:[1.63908461], b:[1.33089619]\n",
            "84 loss is [0.14895993], parameters w:[1.64608042], b:[1.33518136]\n",
            "85 loss is [0.14259603], parameters w:[1.65293366], b:[1.33935092]\n",
            "86 loss is [0.13651098], parameters w:[1.65964736], b:[1.34340777]\n",
            "87 loss is [0.13069234], parameters w:[1.66622448], b:[1.34735477]\n",
            "88 loss is [0.12512823], parameters w:[1.67266792], b:[1.3511947]\n",
            "89 loss is [0.11980733], parameters w:[1.6789805], b:[1.35493028]\n",
            "90 loss is [0.1147188], parameters w:[1.68516501], b:[1.35856415]\n",
            "91 loss is [0.10985231], parameters w:[1.69122414], b:[1.36209889]\n",
            "92 loss is [0.10519798], parameters w:[1.69716055], b:[1.36553703]\n",
            "93 loss is [0.10074639], parameters w:[1.70297682], b:[1.36888101]\n",
            "94 loss is [0.09648853], parameters w:[1.70867551], b:[1.37213324]\n",
            "95 loss is [0.0924158], parameters w:[1.71425908], b:[1.37529605]\n",
            "96 loss is [0.08851999], parameters w:[1.71972997], b:[1.37837173]\n",
            "97 loss is [0.08479327], parameters w:[1.72509054], b:[1.38136249]\n",
            "98 loss is [0.08122813], parameters w:[1.73034313], b:[1.38427052]\n",
            "99 loss is [0.07781743], parameters w:[1.73549], b:[1.38709792]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in our 99th epoch the loss value is so close to our y-value which is shown below code cell after running it."
      ],
      "metadata": {
        "id": "eU_0v_DxMQb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwD3forhGSJz",
        "outputId": "4fe49b3d-a297-4c5d-c319-22011373027f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.08811048]\n",
            " [ 2.45510356]\n",
            " [-0.26924857]\n",
            " [ 0.61393311]\n",
            " [ 0.79854911]\n",
            " [ 0.32728019]\n",
            " [-0.06469006]\n",
            " [-0.5236279 ]\n",
            " [-0.36309236]\n",
            " [ 0.25226907]] [[-0.70824293]\n",
            " [ 6.37818515]\n",
            " [ 0.92948089]\n",
            " [ 2.69584425]\n",
            " [ 3.06507624]\n",
            " [ 2.12253841]\n",
            " [ 1.33859791]\n",
            " [ 0.42072223]\n",
            " [ 0.74179331]\n",
            " [ 1.97251616]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XMZNNChrIcrJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}